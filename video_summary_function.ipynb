{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wbwfwdw7-7k5",
        "outputId": "0b2f8e4c-c587-495f-8cbf-1d8a364111ee"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m47.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m90.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m77.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.6.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.15.1 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.30.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install youtube_transcript_api"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_BQYx5OU-_D-",
        "outputId": "2be5dc1c-7d52-4cfa-faae-8685143cdfc4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting youtube_transcript_api\n",
            "  Downloading youtube_transcript_api-0.6.1-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from youtube_transcript_api) (2.27.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->youtube_transcript_api) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->youtube_transcript_api) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->youtube_transcript_api) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->youtube_transcript_api) (3.4)\n",
            "Installing collected packages: youtube_transcript_api\n",
            "Successfully installed youtube_transcript_api-0.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "Fmuz9rS3-1X8"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "from youtube_transcript_api import YouTubeTranscriptApi\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_video(video_link):\n",
        "    if \"youtube.com/watch?v=\" not in video_link:\n",
        "        print(\"Invalid YouTube video link!\")\n",
        "        return\n",
        "\n",
        "    # Extract video ID from the YouTube link\n",
        "    video_id = video_link.split(\"=\")[1]\n",
        "\n",
        "    # Get the transcript of the YouTube video\n",
        "    transcript = YouTubeTranscriptApi.get_transcript(video_id)\n",
        "\n",
        "    # Combine the transcript into a single string\n",
        "    result = \"\"\n",
        "    for i in transcript:\n",
        "        result += ' ' + i['text']\n",
        "\n",
        "    # Initialize the summary pipeline\n",
        "    summarizer = pipeline('summarization')\n",
        "\n",
        "    # Measure the inference time\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Summarize the text\n",
        "    num_iters = int(len(result) / 1000)\n",
        "    summarized_text = []\n",
        "    for i in range(num_iters + 1):\n",
        "        start = i * 1000\n",
        "        end = (i + 1) * 1000\n",
        "        input_text = result[start:end]\n",
        "        out = summarizer(input_text)\n",
        "        out = out[0]\n",
        "        out = out['summary_text']\n",
        "        summarized_text.append(out)\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    # Calculate the inference time\n",
        "    inference_time = end_time - start_time\n",
        "\n",
        "    # Save the summarized text in a TXT file\n",
        "    txt_filename = \"summarized_text.txt\"\n",
        "    with open(txt_filename, 'w') as txt_file:\n",
        "        for text in summarized_text:\n",
        "            txt_file.write(text + '\\n')\n",
        "\n",
        "    # Print the summarized text and inference time\n",
        "    print(\"Summarized text:\")\n",
        "    for text in summarized_text:\n",
        "        print(text)\n",
        "\n",
        "    print('Inference time:', inference_time, \"seconds\")\n",
        "    print(\"Text saved as:\", txt_filename)"
      ],
      "metadata": {
        "id": "ZZtVu3Vq_QoB"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "video_link = \"https://www.youtube.com/watch?v=E9lAeMz1DaM\"\n",
        "summarize_video(video_link)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jE8T6YQ2BDsB",
        "outputId": "2cb11100-75a8-456a-9528-2ec005d68c9d"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Your max_length is set to 142, but your input_length is only 50. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=25)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summarized text:\n",
            " i don't know what that dream is that you have i. don't care how disappointing it might have been as you've been working toward that dream that you're holding in your mind that it's possible [Music] you are not alone, say i. Don't forget that you are holding in the mind of that dream .\n",
            "Inference time: 10.97602128982544 seconds\n",
            "Text saved as: summarized_text.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rpLoAM3iDKXY"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rIhuETKFEqPp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}